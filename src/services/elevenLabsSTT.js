// ElevenLabs Speech-to-Text service
import { API_KEYS } from '../../config.js';

export class ElevenLabsSTTService {
  constructor() {
    this.apiKey = API_KEYS.ELEVENLABS_API_KEY;
    this.baseUrl = 'https://api.elevenlabs.io/v1';
    this.hasApiKey = this.apiKey && this.apiKey !== 'your_elevenlabs_api_key_here';
    
    console.log(`üîë ElevenLabs API Key loaded: ${this.hasApiKey ? 'YES' : 'NO'}`);
    if (this.hasApiKey) {
      console.log(`üîë API Key: ${this.apiKey.substring(0, 10)}...`);
    }
    
    // Rate limiting and queuing
    this.requestQueue = [];
    this.activeRequests = 0;
    this.maxConcurrentRequests = 1; // Reduced to 1 to avoid rate limiting
    this.requestDelay = 1500; // 1.5 second delay between requests
    this.lastRequestTime = 0;
  }

  async transcribeAudio(audioBuffer, language = 'en') {
    if (!this.hasApiKey) {
      throw new Error('ElevenLabs API key not provided');
    }

    // Check minimum audio length (ElevenLabs requires at least 0.5 seconds)
    const minAudioLength = 8000; // ~0.5 seconds at 16kHz
    if (audioBuffer.byteLength < minAudioLength) {
      console.log(`‚ö†Ô∏è Audio too short for ElevenLabs STT: ${audioBuffer.byteLength} bytes (minimum: ${minAudioLength} bytes)`);
      throw new Error(`Audio too short: ${audioBuffer.byteLength} bytes (minimum: ${minAudioLength} bytes)`);
    }

    try {
      console.log(`üé§ ElevenLabs STT: Starting transcription (${audioBuffer.byteLength} bytes)`);
      
      // Convert audio to proper WAV format
      console.log('üéµ Converting audio to WAV format');
      const wavBuffer = await this.convertWebMToWav(audioBuffer);
      const audioBlob = new Blob([wavBuffer], { type: 'audio/wav' });
      
      // Create form data with proper file format
      const formData = new FormData();
      formData.append('file', audioBlob, 'audio.wav');
      formData.append('model_id', 'scribe_v1');
      formData.append('language', language);
      formData.append('response_format', 'json');
      formData.append('temperature', '0.2');

      const response = await fetch(`${this.baseUrl}/speech-to-text`, {
        method: 'POST',
        headers: {
          'xi-api-key': this.apiKey
          // Don't set Content-Type, let fetch set it with boundary for FormData
        },
        body: formData
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`ElevenLabs STT API error: ${response.status} ${response.statusText} - ${errorText}`);
      }

      const result = await response.json();
      console.log(`‚úÖ ElevenLabs STT: Transcription completed - "${result.text}"`);
      console.log(`üîç Full STT response:`, JSON.stringify(result, null, 2));
      
      return {
        text: result.text || '',
        confidence: result.confidence || 0.9,
        isFinal: true,
        provider: 'elevenlabs'
      };
    } catch (error) {
      console.error('‚ùå ElevenLabs STT error:', error);
      throw error; // Don't fall back to mock, let the error propagate
    }
  }


  async convertWebMToWav(audioBuffer) {
    try {
      // For now, we'll create a simple WAV file with the raw audio data
      // This is a simplified approach - in production you'd want to properly decode WebM/Opus
      const sampleRate = 16000; // 16kHz sample rate
      const numChannels = 1; // Mono
      const bitsPerSample = 16;
      const blockAlign = numChannels * bitsPerSample / 8;
      const byteRate = sampleRate * blockAlign;
      
      // Use the raw audio buffer size (this is a simplified approach)
      const dataSize = audioBuffer.byteLength;
      const fileSize = 44 + dataSize;

      // Create WAV header
      const buffer = new ArrayBuffer(44 + dataSize);
      const view = new DataView(buffer);

      // RIFF header
      view.setUint32(0, 0x46464952, true); // "RIFF"
      view.setUint32(4, fileSize - 8, true);
      view.setUint32(8, 0x45564157, true); // "WAVE"

      // fmt chunk
      view.setUint32(12, 0x20746d66, true); // "fmt "
      view.setUint32(16, 16, true); // fmt chunk size
      view.setUint16(20, 1, true); // audio format (PCM)
      view.setUint16(22, numChannels, true);
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, byteRate, true);
      view.setUint16(32, blockAlign, true);
      view.setUint16(34, bitsPerSample, true);

      // data chunk
      view.setUint32(36, 0x61746164, true); // "data"
      view.setUint32(40, dataSize, true);

      // Copy audio data
      const audioView = new Uint8Array(audioBuffer);
      const wavView = new Uint8Array(buffer, 44);
      wavView.set(audioView);

      console.log(`üéµ Converted WebM to WAV: ${audioBuffer.byteLength} bytes -> ${buffer.byteLength} bytes`);
      
      return buffer;
    } catch (error) {
      console.error('‚ùå Error converting WebM to WAV:', error);
      // Fallback to simple conversion
      return this.convertToWav(audioBuffer);
    }
  }

  convertToWav(audioBuffer) {
    // Convert raw audio buffer to WAV format
    const sampleRate = 16000; // 16kHz sample rate
    const numChannels = 1; // Mono
    const bitsPerSample = 16;
    const blockAlign = numChannels * bitsPerSample / 8;
    const byteRate = sampleRate * blockAlign;
    const dataSize = audioBuffer.byteLength;
    const fileSize = 44 + dataSize;

    // Create WAV header
    const buffer = new ArrayBuffer(44 + dataSize);
    const view = new DataView(buffer);

    // RIFF header
    view.setUint32(0, 0x46464952, true); // "RIFF"
    view.setUint32(4, fileSize - 8, true);
    view.setUint32(8, 0x45564157, true); // "WAVE"

    // fmt chunk
    view.setUint32(12, 0x20746d66, true); // "fmt "
    view.setUint32(16, 16, true); // fmt chunk size
    view.setUint16(20, 1, true); // audio format (PCM)
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, byteRate, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitsPerSample, true);

    // data chunk
    view.setUint32(36, 0x61746164, true); // "data"
    view.setUint32(40, dataSize, true);

    // Copy audio data
    const audioView = new Uint8Array(audioBuffer);
    const wavView = new Uint8Array(buffer, 44);
    wavView.set(audioView);

    return buffer;
  }



  async translateText(text, targetLanguage = 'es', voiceId = null) {
    try {
      console.log(`\nüåê ===== TRANSLATION REQUEST =====`);
      console.log(`üìù BEFORE: "${text}"`);
      console.log(`üåç TARGET LANGUAGE: ${targetLanguage}`);
      console.log(`üåê ElevenLabs STT: Translating "${text}" to ${targetLanguage}`);
      
      // Handle empty text
      if (!text || text.trim() === '') {
        console.log('‚ö†Ô∏è Empty text provided for translation, returning empty result');
        return {
          translatedText: '',
          confidence: 0.0,
          audioData: null
        };
      }
      
      // Use Google Translate free endpoint with better parsing
      const response = await fetch(`https://translate.googleapis.com/translate_a/single?client=gtx&sl=auto&tl=${targetLanguage}&dt=t&q=${encodeURIComponent(text)}`);
      const data = await response.json();
      
      if (data && data[0] && data[0].length > 0) {
        // Get the best translation by combining all translation segments
        let translatedText = '';
        let confidence = 0.8;
        
        // Combine all translation segments for better quality
        for (let i = 0; i < data[0].length; i++) {
          if (data[0][i] && data[0][i][0]) {
            translatedText += data[0][i][0];
          }
        }
        
        // Clean up the translation
        translatedText = translatedText.trim();
        
        // If we got a good translation, use it
        if (translatedText && translatedText.length > 0) {
          console.log(`\n‚úÖ ===== TRANSLATION RESULT =====`);
          console.log(`üìù BEFORE: "${text}"`);
          console.log(`üìù AFTER:  "${translatedText}"`);
          console.log(`üåç LANGUAGE: auto ‚Üí ${targetLanguage}`);
          console.log(`üéØ CONFIDENCE: ${confidence}`);
          console.log(`================================\n`);
        } else {
          throw new Error('No translation received');
        }
        
        // Generate audio using ElevenLabs TTS if API key is available
        let audioData = null;
        if (this.hasApiKey) {
          try {
            console.log(`üéµ Generating audio with ElevenLabs TTS for: "${translatedText}"`);
            const defaultVoiceId = voiceId || 'pNInz6obpgDQGcFmaJgB'; // Default ElevenLabs voice
            audioData = await this.generateAudio(translatedText, defaultVoiceId);
            console.log(`‚úÖ Audio generated successfully`);
          } catch (audioError) {
            console.warn('‚ö†Ô∏è Audio generation failed:', audioError.message);
          }
        }
        
        return {
          translatedText: translatedText,
          confidence: 0.8,
          audioData: audioData
        };
      } else {
        throw new Error('Translation API failed');
      }
    } catch (error) {
      console.log(`\n‚ö†Ô∏è ===== FALLBACK TRANSLATION =====`);
      console.log(`üìù BEFORE: "${text}"`);
      console.log(`üåç TARGET LANGUAGE: ${targetLanguage}`);
      console.log(`‚ùå API ERROR: ${error.message}`);
      
      // Fallback translation for demo purposes
      const fallbackTranslations = {
        'Richard. William. Orange. More.': 'Ricardo. Guillermo. Naranja. M√°s.',
        'Richard. William. Orange.': 'Ricardo. Guillermo. Naranja.',
        'Hello': 'Hola',
        'Thank you': 'Gracias',
        'Good morning': 'Buenos d√≠as',
        'How are you': '¬øC√≥mo est√°s?'
      };
      
      const fallbackTranslation = fallbackTranslations[text] || `[Translated: ${text}]`;
      
      console.log(`\n‚úÖ ===== FALLBACK RESULT =====`);
      console.log(`üìù BEFORE: "${text}"`);
      console.log(`üìù AFTER:  "${fallbackTranslation}"`);
      console.log(`üåç LANGUAGE: en ‚Üí ${targetLanguage}`);
      console.log(`üéØ CONFIDENCE: 0.8 (fallback)`);
      console.log(`================================\n`);
      
      // Generate audio using ElevenLabs TTS for fallback translations too
      let audioData = null;
      if (this.hasApiKey) {
        try {
          console.log(`üéµ Generating fallback audio with ElevenLabs TTS for: "${fallbackTranslation}"`);
          audioData = await this.generateAudio(fallbackTranslation, options?.voiceId);
          console.log(`‚úÖ Fallback audio generated successfully`);
        } catch (audioError) {
          console.warn('‚ö†Ô∏è Fallback audio generation failed:', audioError.message);
        }
      }
      
      return {
        translatedText: fallbackTranslation,
        confidence: 0.8,
        audioData: audioData
      };
    }
  }

  async generateAudio(text, voiceId = null) {
    if (!this.hasApiKey) {
      console.log('‚ö†Ô∏è ElevenLabs TTS: API key not provided, skipping audio generation');
      return null;
    }

    // Use queued request to handle rate limiting
    return new Promise((resolve) => {
      this.requestQueue.push({
        text,
        voiceId,
        resolve,
        timestamp: Date.now()
      });
      this.processQueue();
    });
  }

  async processQueue() {
    if (this.activeRequests >= this.maxConcurrentRequests || this.requestQueue.length === 0) {
      return;
    }

    const now = Date.now();
    const timeSinceLastRequest = now - this.lastRequestTime;
    
    if (timeSinceLastRequest < this.requestDelay) {
      // Wait before processing next request
      setTimeout(() => this.processQueue(), this.requestDelay - timeSinceLastRequest);
      return;
    }

    const request = this.requestQueue.shift();
    if (!request) return;

    this.activeRequests++;
    this.lastRequestTime = now;

    try {
      console.log(`üéµ Processing TTS request: "${request.text.substring(0, 50)}..."`);
      
      // Use custom voice ID if provided, otherwise use default
      const voiceId = request.voiceId || '21m00Tcm4TlvDq8ikWAM';
      console.log(`üé§ Using voice ID: ${voiceId}`);
      
      const response = await fetch(`${this.baseUrl}/text-to-speech/${voiceId}`, {
        method: 'POST',
        headers: {
          'xi-api-key': this.apiKey,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          text: request.text,
          model_id: 'eleven_multilingual_v2',
          voice_settings: {
            stability: 0.5,
            similarity_boost: 0.5
          }
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.warn(`‚ö†Ô∏è ElevenLabs TTS API error: ${response.status} ${response.statusText} - ${errorText}`);
        request.resolve(null);
      } else {
        const audioBuffer = await response.arrayBuffer();
        const audioArray = Array.from(new Uint8Array(audioBuffer));
        console.log(`‚úÖ ElevenLabs TTS: Generated ${audioArray.length} bytes of audio`);
        request.resolve(audioArray);
      }
    } catch (error) {
      console.warn('‚ö†Ô∏è ElevenLabs TTS error:', error.message);
      request.resolve(null);
    } finally {
      this.activeRequests--;
      // Process next request in queue
      setTimeout(() => this.processQueue(), 100);
    }
  }


  async getUsage() {
    if (!this.hasApiKey) {
      return { characters_used: 0, characters_limit: 10000 };
    }

    try {
      const response = await fetch(`${this.baseUrl}/user`, {
        headers: {
          'xi-api-key': this.apiKey
        }
      });

      if (!response.ok) {
        throw new Error(`ElevenLabs API error: ${response.status}`);
      }

      const data = await response.json();
      return {
        characters_used: data.subscription?.character_count || 0,
        characters_limit: data.subscription?.character_limit || 10000
      };
    } catch (error) {
      console.error('‚ùå Failed to get ElevenLabs usage:', error);
      return { characters_used: 0, characters_limit: 10000 };
    }
  }
}

export const elevenLabsSTTService = new ElevenLabsSTTService();
